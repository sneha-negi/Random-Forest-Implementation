#Random Forest Implementation
=======================

##Overview
--------

This project offers a Random Forest implementation from scratch, leveraging the `DecisionTreeClassifier` from `sklearn` as the base model. It covers essential ensemble learning techniques, such as bagging, bootstrapping, and hard voting, showcasing how these methods can combine individual decision trees to improve model accuracy.

##Dataset
-------

The project uses the Titanic dataset from Kaggle, located in the `dataset/` folder. It contains data on passenger information, and preprocessing has been applied to handle missing values, feature encoding, and normalization, preparing it for training.

##Project Structure
-----------------

-   **`dataset/`**: Contains the preprocessed Titanic dataset CSV file.
-   **`utils.py`**: Includes the `error_score` function, which calculates the error score between predicted and actual values, useful for evaluating model performance.

### Core Python Files

-   **`bagging.py`**: Implements the bagging method, where multiple decision trees are trained on different subsets of the training data, aggregated to form predictions.
-   **`bootstrapping.py`**: Handles bootstrapping, which creates multiple subsets of data using sampling with replacement, forming the base for training individual trees.
-   **`hard_voting.py`**: Implements hard voting, combining predictions from multiple trees by selecting the most frequent predicted class.
-   **`random_forest_bagging.py`**: Combines multiple decision trees using bagging to form a Random Forest ensemble.
-   **`random_forest_bootstrapping.py`**: Creates a Random Forest using bootstrapping, training trees on different bootstrapped datasets.
-   **`predict.py`**: Executes predictions using the trained Random Forest model and visualizes model performance. It produces a Matplotlib graph showing how the test error varies with the number of trees.

##Output
------

The output generated by `predict.py` is a graphical representation of "Number of Trees vs. Test Error," illustrating how increasing the number of trees in the ensemble affects prediction accuracy. This helps in understanding the impact of ensemble size on model performance.
